
smallDataset{
  inputLayoutFile     = "C:\\Projects\\spark\\testsparksql\\input\\memberlayout.csv"
  claimdata           = "C:\\Projects\\spark\\testsparksql\\input\\claimdata.csv"
  memberdata          = "C:\\Projects\\spark\\testsparksql\\input\\memberdata.csv"
  providerdata        = "C:\\Projects\\spark\\testsparksql\\input\\providerdata.csv"
  hasHeader      = "true"
  dataDelimiter      = ","
  lineDelimiter      = "\\n"
  outputDirectory     = "C:\\Projects\\spark\\testsparksql\\output\\"
  samplePercent   = "5"
}

membersProducer {
records = 100000
file_path = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\MemberData.csv"
dest_path = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\"
}

providersProducer {
  records = 100000
  file_path = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\providerData.csv"
  dest_path = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\"
}



claimsProducer {
  records = 500000
  memberFilePath = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\MemberData.csv"
  providerFilePath = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\providerData.csv"
  file_path = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\Claimdata.csv"
  dest_path = "C:\\Projects\\spark\\testsparksql\\src\\main\\resources\\"
}

kafka {
  topic = "test"
  server = "localhost:9092"
  clientId = "kafkaTestId"
}

spark {
  ip = "local[*]"
  executor.memory = "2G"
  app.name = "KafkaStream"
  cores = "2"
}

jdbcSqlServer{

  server= "localhost"
  user = "sa"
  password = "P@ssword12"
  DatabaseName = "AdventureWorks2016CTP3"
  BulkLoadBatchSize = "1000"

}

zookeeper.server = "localhost:2181"